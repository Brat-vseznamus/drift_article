% \documentclass[8pt, a4paper, twocolumn]{article}
\documentclass[12pt, a4paper]{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{indentfirst}
\usepackage{graphicx}
\usepackage{mathtext}
\usepackage[T1,T2A]{fontenc}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{stmaryrd}
\usepackage{a4wide}

\lstset{basicstyle=\ttfamily\footnotesize,breaklines=true}

\title{\text{Additive drift with tail bounds}}

% \date{04-08-2021}

\newcommand\setItemnumber[1]{\setcounter{enumi}{\numexpr#1-1\relax}}

\newcommand{\gfun}{\mathbf{g}}

\newcounter{casenum}
\newenvironment{caseof}{\setcounter{casenum}{1}}{\vskip.5\baselineskip}
\newcommand{\case}[2]{\vskip.5\baselineskip\par\noindent {\bfseries Case \arabic{casenum}:} #1\\#2\addtocounter{casenum}{1}}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\theoremstyle{remark}
\newtheorem*{remark}{Remark}

\newcommand{\expx}[1]{e^{-|x|^{#1}}}
\newcommand{\expxpoz}[1]{e^{-x^{#1}}}
\newcommand{\infint}[1]{\int_{-\infty}^{+\infty} #1 \, dx}
\newcommand{\infintpoz}[1]{\int_{0}^{+\infty} #1 \, dx}
\newcommand{\der}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\cm}{coming soon\dots}

\newcommand{\underrel}[2]{\mathrel{\mathop{#2}\limits_{#1}}}

\author{Vihnin F.
    \and Antipov D.
    \and Sinyachenko N.}


\begin{document}
\maketitle

\section{Introduction}
\cm

\section{Existing tools}
\cm
\subsection{Negative drift}
\begin{theorem}[Drift Theorem for Lower Bounds] \label{thm:neat}
    Let $\{X_t\}_{t \ge 0}$ be a Markow process over a finite set of states $\mathbb{S}$, and $\mathbf{g} : \mathbb{S} \rightarrow \mathbb{R}$ a function that assigns to every state a non-negative real numbers. Pick two real numbers $a$ and $b$ such that $a < b$ and let the random variable $T$ denote
    the earliest point in time $t \ge 0$ where $\mathbf{g}(X_t) \leq a$ holds.
    
    If there are constants $\lambda > 0$ and $D \ge 1$ and a $p$ taking only positive values, for which the following four conditions hold
    \renewcommand\labelenumi{(\theenumi)}
    \begin{enumerate}
        \item $\gfun(X_0) \ge b$,
        \item $\forall t \ge 0 : E\left[e^{-\lambda(\gfun(X_{t + 1} - \gfun(X_t)))} |\ X_t,\ \gfun(X_t) < b\right] \leq 1 - \frac{1}{p} = \rho$,
        \item $\forall t \ge 0 : E\left[e^{-\lambda(\gfun(X_{t + 1} - b))} |\ X_t,\ \gfun(X_t) \ge b\right] \leq D$,
    \end{enumerate}
    then for all time bounds $B \ge 0$ the probability that $T$ exceeds $B$ is at most:

    \begin{equation}
        \Pr[T \leq B] \leq e^{\lambda(a - b)} \cdot B \cdot D \cdot p
    \end{equation}
\end{theorem}

To prove this theorem we use the following lemma from \cite{Hajek 82}

\begin{lemma}[Lemma 2.8 in \cite{Hajek 82}] \label{lm:haj}
    If conditions 2 and 3 are met then for all $t \geq 0$:

    $$\Pr[\gfun(X_t) \leq a] \leq \rho^te^{\lambda(a - \gfun(X_0))} + \frac{1 - \rho^t}{1 - \rho} D e^{\lambda(a - b)}$$
    
\end{lemma}

\begin{proof}[Proof of Theorem \ref{thm:neat}]
    
    If $T = B$, then we have $\gfun(X_k) \leq a$ and for all $t < B$ we have $\gfun(X_t) > a$. Hence, we compute

    \begin{align*}
        \Pr[T \leq B] &= \sum_{k=1}^{B} \Pr[T = k] \\
        &= \sum_{k=1}^{B} \Pr[\gfun(X_k) \leq a \land \gfun(X_{k - 1}) > a \land ... \land \gfun(X_0) > a)] \\
        & \leq \sum_{k=1}^{B} \Pr[\gfun(X_k) \leq a].
    \end{align*}

    By Lemma $\ref{lm:haj}$

    \begin{align*}
        \Pr[T \leq B] & \leq \sum_{k=1}^{B} \left(\rho^k e^{\lambda(a - \gfun(X_0))} + \frac{1 - \rho^k}{1 - \rho} D e^{\lambda(a - b)}\right)\\
        & = \rho \left(\frac{1 - \rho^B}{1 - \rho}\right) e^{\lambda a}\left(e^{-\lambda\gfun(X_0)} - \frac{D}{1 - \rho} e^{-\lambda b}\right) + e^{\lambda(a - b)} \frac{B D}{1 - \rho}.
    \end{align*}

    Since $\gfun(X_0) \ge b$ we have

    \begin{align*}
        \Pr[T \leq B] & \leq \rho \frac{1 - \rho^B}{1 - \rho} e^{\lambda (a - b)}\left(1 - \frac{D}{1 - \rho}\right) + e^{\lambda(a - b)} \frac{B D}{1 - \rho}
    \end{align*}
    
    Since $\frac{D}{1 - \rho} \geq D \geq 1 $, we further compute
    \begin{align*}
        \Pr[T \leq B] \leq e^{\lambda(a - b)} \frac{B D}{1 - \rho} = e^{\lambda(a - b)} \cdot B \cdot D \cdot p
    \end{align*}

\end{proof}
\subsection{Theorem for sub-gaussian processes}
\cm
\subsection{Comparing requirements}
To use some versions of drift theorems, we need some requirements on the sequence of random variables to be satisfied, such as the following:
$$
\exists \delta \geq 0,\ c \in \mathbb{R} \ \forall  t \geq 0 \ \forall \gamma \in [0, \delta]: $$$$
E\left[e^{\gamma (X_{t + 1} - X_{t})}\right] \leq e^{\frac{c \gamma^2}{2}},$$

оr

$$\exists \delta \geq 0,\ p \geq 1 \ \forall t \geq 0: $$$$
E\left[e^{\gamma (X_{t + 1} - X_{t})}\right] \leq 1 - \frac{1}{p}\ .$$


To compare rigors of requirements we need to explore behavior $E[e^{\gamma X}]$ regarding to $\gamma$. Of course this behavior depends on distribution type, hence consider common facts.   

\subsubsection*{Continuos distribution}

Consider the definition of expectation of a function:
\[
E[e^{\gamma X}] = \int_{-\infty}^{+\infty} f(x) e^{\gamma x} \,dx,    
\]

where $f(x)$ is a distribution density function.

\hfill

Let's take the first derivative $\gamma$:

\begin{align*}
\frac{\partial E[e^{\gamma X}]}{\partial \gamma} &= \frac{\partial}{\partial \gamma} \int_{-\infty}^{+\infty} f(x) e^{\gamma x} \, dx = \int_{-\infty}^{+\infty} f(x) \frac{\partial e^{\gamma x}}{\partial \gamma} \, dx = \int_{-\infty}^{+\infty} x f(x) e^{\gamma x} \, dx = \\
&= \int_{-\infty}^{0} x f(x) e^{\gamma x} \, dx + \int_{0}^{+\infty} x f(x) e^{\gamma x} \, dx.
\end{align*}

Note that $\forall x \leq 0 : e^{\gamma x} \leq 1$ and $\forall x \geq 0 : e^{\gamma x} \geq 1 + \gamma x$. Hence because $\forall x : f(x) \geq 0$ 

\begin{align*}
    \int_{0}^{+\infty} x f(x) e^{\gamma x} \, dx &\geq \int_{0}^{+\infty} x f(x) (1 + \gamma x)\, dx    \\
    \int_{-\infty}^{0} x f(x) e^{\gamma x} \, dx &\geq \int_{-\infty}^{0} x f(x)\, dx.    
\end{align*}

And then

\begin{align*}
\frac{\partial E[e^{\gamma X}]}{\partial \gamma} &\geq  \int_{0}^{+\infty} x f(x) (1 + \gamma x)\, dx + \int_{-\infty}^{0} x f(x)\, dx  \\
&= E[X] + \gamma \int_{0}^{+\infty} x^2 f(x) \, dx.
\end{align*}

Denote $\int_{0}^{+\infty} x^2 f(x) \, dx = c \geq 0$ and then if we reintegrate back, we get a restriction on the expectation from below:

\[
E[e^{\gamma X}] = E[1] + \int_0^\gamma \frac{\partial E[e^{\gamma X}]}{\partial \gamma} \, d\gamma \geq 1 + \gamma E[X] + \gamma^2 \frac{c}{2}.    
\]

\subsubsection*{Discrete distribution}

In the same way consider expectation of function by definition:

\begin{align*}
    E[e^{\gamma X}] = \sum_{i = 1}^{+\infty} e^{\gamma x_i} \Pr[x = x_i],
\end{align*}

where $\Pr$ is a probability function.

% \hfill

Denote set $P = \{i \in \mathbb{N}: x_i \geq 0\}$. Hence 

\begin{align*}
    E[e^{\gamma X}] = \sum_{i = 1}^{+\infty} e^{\gamma x_i} Pr[x = x_i] = \sum_{i \in P} e^{\gamma x_i} Pr[x = x_i] + \sum_{i \in \mathbb{N}\setminus P} e^{\gamma x_i} Pr[x = x_i],
\end{align*}

After taking the first derivative we may split terms from $P$ set with other: 
% В таком случае, аналогично возьмем производную и рассмотрим пот отдельности слагаемые из множества P и остальные:

\begin{align*}
    \der{E[e^{\gamma X}]}{\gamma} &= \sum_{i = 1}^{+\infty} \frac{\partial e^{\gamma x_i}}{\partial \gamma} Pr[x = x_i] = \sum_{i = 1}^{+\infty} x_i e^{\gamma x_i} Pr[x = x_i] \\
    &= \sum_{i \in P} x_i e^{\gamma x_i} Pr[x = x_i] + \sum_{i \in \mathbb{N}\setminus P} x_i e^{\gamma x_i} Pr[x = x_i] \\
    &\geq \sum_{i \in P} x_i (1 + {\gamma x_i}) Pr[x = x_i] + \sum_{i \in \mathbb{N}\setminus P} x_i Pr[x = x_i] \\
    &= E[X] + \gamma\sum_{i \in P} x_i^2 Pr[x = x_i]
\end{align*}

Again denote $c = \sum_{i \in P} x_i^2 Pr[x = x_i] \geq 0$ and obtain a result similar to the previous paragraph 

\[
E[e^{\gamma X}] \geq 1 + \gamma E[X] + \gamma^2 \frac{c}{2}  
\]

Because formulas for both cases are same all further conclusions will be same too.

\subsubsection{Impracticability for positive drift}

In this case, the expectation of the exponent cannot be lower than 1, and also in the vicinity of zero it grows faster than any$e^{\frac{\gamma^2 c}{2}}$, because
        
$$\frac{\partial e^{\frac{\gamma^2 c}{2}}}{\partial \gamma} = c\gamma e^{\frac{\gamma^2 c}{2}} = g(\gamma),$$

But $g(0) = 0$, while $E[X] > 0$, therefore, that in any vicinity of zero

$$\forall c > 0 \, \exists \gamma_c > 0 \, \forall \gamma \in (0, \gamma_c) : e^{\frac{\gamma^2 c}{2}} < E[e^{\gamma X}],$$

Hence both of requirements are not feasible.

\subsubsection{Zero drfit case}
\label{sec:zero}

In this section we will consider some specific family of continuos distributions with symmetric distribution density functions, because their expectation equals 0.

\paragraph*{Exponent of polynomial}

Consider function

$$f(x) = c \expx{a},\ a > 0.$$

Compute $c$

\begin{align*}
    \infint{f(x)} &= \infint{c\expx{a}} = 1
\end{align*}

\begin{align*}
    \infint{\expx{a}} = 2\infintpoz{\expxpoz{a}}.
\end{align*}

Let's make a replacement

\begin{align*}
    u &= x^a \\
    du &= dx\,(a x^{a - 1}) = dx\,(a u^{1 - \frac{1}{a}}),
\end{align*}

then

\begin{align*}
    2\infintpoz{\expxpoz{a}} = \frac{2}{a} \infintpoz{e^{-u}u^{\frac{1}{a} - 1}} = \frac{2\Gamma(\frac{1}{a})}{a}.
\end{align*}

We conclude that $c = \frac{a}{2\Gamma(\frac{1}{a})}
$ and 

$$f(x) = \frac{a}{2\Gamma(\frac{1}{a})} \expx{a}.$$

Let's find n-th momentum of $X$ (expectation от $X^n$): 

\begin{align*}
    E[X^n] = \infint{f(x) x^n} = \frac{\Gamma(\frac{n + 1}{a})}{2\Gamma(\frac{1}{a})} (1 + (-1)^n)
\end{align*}

Then we can consider $e^{\gamma X}$ as a infinite series

\begin{align*}
    E[e^{\gamma X}] = E\left[\sum_{n = 0}^{+\infty}\frac{(\gamma X)^n}{n!}\right] &= \sum_{n = 0}^{+\infty}\gamma^n\frac{E[X^n]}{n!} = \sum_{n = 0}^{+\infty}\gamma^{2n}\frac{E[X^{2n}]}{2n!} =\\
    &= \sum_{n = 0}^{+\infty}\frac{(\gamma^2)^{n}}{n!}\left(\frac{E[X^{2n}]n!}{2n!}\right) = \sum_{n = 0}^{+\infty}\frac{(\gamma^2)^{n}}{n!} a_n
\end{align*}

Third equality is satisfied because all odd momentums are 0 due to the symmetry of the function.

Consider sequence $\{\sqrt[n]{a_n}\}$:

\begin{align*}
    A = \lim_{n \to +\infty} \sqrt[n]{a_n} &= \lim_{n \to +\infty} \sqrt[n]{\frac{n! E[X^{2n}]}{2n!}} = 
    \lim_{n \to +\infty} \sqrt[n]{
        \frac{
            \left(\frac{n}{e}\right)^n \sqrt{2\pi n} \
            \Gamma\left(\frac{2n + 1}{a}\right)}{
                \left(\frac{2n}{e}\right)^{2n} \sqrt{4\pi n}\ \Gamma\left(\frac{1}{a}\right)}
        }\\
    &= \lim_{n \to +\infty} \frac{n}{e} \frac{e^2}{4n^2} \sqrt[n]{\Gamma\left(\frac{2n + 1}{a}\right)} \simeq \lim_{n \to +\infty}  \frac{e}{4n} \sqrt[n]{\left(\frac{2n + 1}{ae}\right)^{\frac{2n + 1}{a}}} \\
    &= \lim_{n \to +\infty} \frac{e}{4n} \left(\frac{2n}{ae}\right)^{\frac{2}{a}} = \left(\frac{e^{1 - \frac{2}{a}}}{2^{2 - \frac{2}{a}} a^\frac{2}{a}}\right) \lim_{n \to +\infty} n^{\frac{2}{a} - 1}.
\end{align*}

Let's take a look on convergence of previous sequence regarding to values of the $a$: 
\begin{caseof}
    \case{$a \geq 2$}{
        
        Then $A$ exist, which means existing $C > 0$ with property 
        $\forall n \geq 0 : a_n \leq C^n$.
        Hence $E[e^{\gamma X}] \leq e^{\gamma^2 C}.$
    }
    \case{$a < 2$}{

        Sequence doesn't converge, so you can't say anything about sub-gausality. 
    }
\end{caseof}

We can compute same limit for $\{\sqrt[n]{\frac{a_n}{n!}}\}$ and threshold regarding to $a$ is 1, where only for $a \geq 1$ series converge.

In this case we need to explore behavior of series when $a \in (1, 2)$. By another definition of sub-gaussian process

$$\exists K > 0 \ \forall p \geq 1: \left(E[X^p]\right)^{\frac{1}{p}} \leq K \sqrt{p}.$$ 


Let's compute (assume $p\ \vdots\ 2$)

\begin{align*}
    E[X^p] &= \frac{\Gamma\left(\frac{p + 1}{a}\right)}{\Gamma\left(\frac{1}{a}\right)} \\
    E[X^p]^\frac{1}{p} &\underrel{p \to +\infty} {=} \left(\frac{p}{e a}\right)^{\frac{p}{a} \frac{1}{p}} = \mathcal{O}(p^\frac{1}{a}).
\end{align*}

For $a \in (1, 2)$ obviously that asymptotics is greater that square root. Hence sub-gausality is not feasible.

\subsubsection{Processes with negative drift} 

In this case, similarly to the positive drift case, it is the summand equal to the expectation that makes the main contribution to the derivative in some neighborhood of zero, since

$$\frac{\partial E[e^{\gamma x}]}{\partial \gamma} \Big|_{\gamma = 0} = E[X] < 0.$$

So both requirements are feasible since:

$$\exists \gamma_0 > 0 : \forall \gamma \in (0, \gamma_0) : E[e^{\gamma X}] < 1.$$

So proving that the process is sub-gaussian is tantamount to finding $p$ in the second requirements.

\textbf{*РАССУЖДЕНИЯ ПРО П И КАРТИНКИ*}


\subsection*{Заключение}

In the case of $\textbf{\text{zero}}$ expectation we cannot always guarantee subgausality, which has been proved by the example of series of even functions $\expx{a}$.

In the case of $\textbf{\text{positive}}$ expectation both criteria are not feasible, and if it is $\textbf{\text{negative}}$, then both are feasible.

\section{Tail bounds}
\subsection{Upper bounds}
Consider a process $\{X_t\}_{t \in \mathbb{N}}$ with positive drift (i.e. $E[X_{t + 1} - X_t |\ X_t = s] \geq 0$ for all $s$) and another process $\{Y_t\}_{t \in \mathbb{N}}$ such that $Y_t = X_t - \varepsilon t$ and it has negative drift (i.e. $E[Y_{t + 1} - Y_t |\ Y_t = s] \leq 0$ for all $s$).

Our aim is to bound the probability $\Pr[T_X \leq t_0]$, where $T_X$ is the first time when $X_t \geq b$ for some $b > X_0$ and for $t_0$. We first note that 

\begin{align*}
    \Pr[T_X \leq t_0] \leq \Pr[X_{t_0} \geq b] \leq \Pr[Y_{t_0} \geq b - \varepsilon t_0].
\end{align*}

Since $\{Y_t\}_{t \in \mathbb{N}}$ is a process with negative drift, it is a subject to theorem \cite{}:

\begin{align*}
    \Pr[Y_{t_0} \geq b - \varepsilon t_0] \leq t_0 D p e^{-\gamma(b - \varepsilon t_0 - a)},
\end{align*}

which also implies

\begin{align*}
    \Pr[T_X \leq t_0] \leq t_0 D p e^{-\gamma(b - \varepsilon t_0 - a)}.
\end{align*}

Let $t_0 := \frac{\kappa (b - a)}{\varepsilon}$. 
\begin{align*}
    \Pr\left[T_X \leq \frac{\kappa (b - a)}{\varepsilon}\right] \leq \frac{\kappa (b - a)}{\varepsilon} D p e^{-\gamma(1 - \kappa)(b - a)}.
\end{align*}

For example we can use $\kappa = \frac{1}{2}$ and obtain

\begin{align*}
    \Pr\left[T_X > \frac{(b - a)}{2\varepsilon}\right] \leq \frac{(b - a)}{2\varepsilon} D p e^{-\gamma\frac{(b - a)}{2}}
\end{align*}

\subsection{Lower bounds}
% Lets take a look on process $\{X_t\}$ and its derivative process $\{Y_t\}$, where $Y_t = \varepsilon t - X_t$ and it has negative drift.

% Then we want to bounds $\Pr[T_X > t_0]$, where $T_X$ - first time $X_t \geq b$. 

% \begin{align*}
%     \Pr[T_X > t_0] \leq \Pr[X_{t_0} < b] \leq \Pr[Y_{t_0} > \varepsilon t_0 - b]
% \end{align*}

% Then, because $\{Y_t\}$ - process with negative drift, we may use the corresponding theorem *label*:

% \begin{align*}
%     \Pr[Y_t > \varepsilon t_0 - b] \leq t_0 D p e^{-\gamma(\epsilon t_0 - b + a)}
% \end{align*}

% Lets rewrite some to $T_X$: 

% \begin{align*}
%     \Pr[T_X > t_0] \leq t_0 D p e^{-\gamma(\epsilon t_0 - b + a)}
% \end{align*}

% Replace $t_0$ with another expression:

% \begin{align*}
%     t_0 := \frac{\kappa (b - a)}{\varepsilon}
% \end{align*}

% And then:

% \begin{align*}
%     \Pr\left[T_X > \frac{\kappa (b - a)}{\varepsilon}\right] \leq \frac{\kappa (b - a)}{\varepsilon} D p e^{-\gamma(\kappa - 1)(b - a)}
% \end{align*}

% For example use $\kappa = \frac{3}{2}$:

% \begin{align*}
%     \Pr\left[T_X > \frac{3 (b - a)}{2\varepsilon}\right] \leq \frac{3(b - a)}{2\varepsilon} D p e^{-\gamma\frac{(b - a)}{2}}
% \end{align*}

% ПОВТОРИТЬ ЕЩЕ РАЗ ПРЕДЫДУЩИЙ ПАРАГРАФ

\section{Time bounds for process with variable $\lambda$}
\cm
\subsection{Case when $\lambda$ is random value}
Consider the total progress $\Delta X$ by $T$ iterations. It is a sum of all progresses $\Delta X_i$ made in previous iteration
\[
    \Delta X = \sum_{i = 0}^{T} \Delta X_i
\]

By Vald's equality we have 

\[
    E[\Delta X] = E\left[\sum_{i = 1}^{T} \Delta X_i\right].      
\]

If ew assume that $E[\Delta X_i]$ is a constant or at least we can give relatively sharp bounds lower and upper bounds on it, then
\[
    E[\Delta X] = E\left[\sum_{i = 1}^{T} \Delta X_i\right] \thickapprox E[T] E[\Delta X_i].      
\]

Let $\Lambda$ be the total cost, which is total number of fitness evaluations (the fitness evaluations in iteration $i$ equals $\lambda_i$). Since $\lambda$ is random variable, each $\lambda_i$ has the same expectation. Therefore, we have
\[
    E\left[\Lambda\right] = E\left[\sum_{i = 1}^{T} \lambda_i\right] = E[T] E[\lambda] \thickapprox \frac{E[\Delta X] E[\lambda]}{E[\Delta X_i]}
\]

If we denote $\vartheta_\lambda = \frac{E[\Delta X_i]}{E[\lambda]}$, which is the expected progress per fitness evaluation, then the previous equation is simplified to
\[
        E[\Lambda] \thickapprox \frac{E[\Delta X]}{\vartheta_\lambda}
\]
% $\vartheta_{\lambda, X_t} = \nu (\lambda, X_t)$

\section{Conclusion}
\cm
\end{document}