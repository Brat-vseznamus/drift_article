\documentclass[12pt, a4paper]{article}
% \documentclass[8pt, a4paper, twocolumn]{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{indentfirst}
\usepackage{graphicx}
\usepackage{mathtext}
\usepackage[T1,T2A]{fontenc}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{stmaryrd}
\usepackage{a4wide}
\usepackage{pgfplots}
\usepackage{tikz}
\usepackage{subcaption}

\lstset{basicstyle=\ttfamily\footnotesize,breaklines=true}
\definecolor{darkGreen}{RGB}{31, 140, 47}
\definecolor{darkBlue}{RGB}{31, 53, 125}   
\title{\text{Additive drift with tail bounds}}

% \date{04-08-2021}

\newcommand\setItemnumber[1]{\setcounter{enumi}{\numexpr#1-1\relax}}

\newcommand{\gfun}{\mathbf{g}}

\newcounter{casenum}
\newenvironment{caseof}{\setcounter{casenum}{1}}{\vskip.5\baselineskip}
\newcommand{\case}[2]{\vskip.5\baselineskip\par\noindent {\bfseries Case \arabic{casenum}:} #1\\#2\addtocounter{casenum}{1}}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\theoremstyle{remark}
\newtheorem*{remark}{Remark}

\newcommand{\expx}[1]{e^{-|x|^{#1}}}
\newcommand{\expxpoz}[1]{e^{-x^{#1}}}
\newcommand{\infint}[1]{\int_{-\infty}^{+\infty} #1 \, dx}
\newcommand{\infintpoz}[1]{\int_{0}^{+\infty} #1 \, dx}
\newcommand{\der}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\cm}{coming soon\dots}

\newcommand{\underrel}[2]{\mathrel{\mathop{#2}\limits_{#1}}}

\author{Vihnin F.
    \and Antipov D.
    \and Sinyachenko N.}


\begin{document}
\maketitle

\section{Introduction}
\cm

\section{Existing tools}
\cm
\subsection{Negative drift}
\begin{theorem}[Drift Theorem for Lower Bounds] \label{thm:neat}
    Let $\{X_t\}_{t \ge 0}$ be a Markow process over a finite set of states $\mathbb{S}$, and $\mathbf{g} : \mathbb{S} \rightarrow \mathbb{R}$ a function that assigns to every state a non-negative real numbers. Pick two real numbers $a$ and $b$ such that $a < b$ and let the random variable $T$ denote
    the earliest point in time $t \ge 0$ where $\mathbf{g}(X_t) \leq a$ holds.

    If there are constants $\lambda > 0$, $D \ge 1$, and a $p > 1$ taking only positive values, for which the following four conditions hold
    \renewcommand\labelenumi{(\theenumi)}
    \begin{enumerate}
        \item $\gfun(X_0) \ge b$,
        \item $\forall t \ge 0 E\left[e^{-\lambda(\gfun(X_{t + 1} - \gfun(X_t)))} |\ X_t,\ \gfun(X_t) < b\right] \leq 1 - \frac{1}{p} =: \rho$,
        \item $\forall t \ge 0 E\left[e^{-\lambda(\gfun(X_{t + 1} - b))} |\ X_t,\ \gfun(X_t) \ge b\right] \leq D$,
    \end{enumerate}
    then for all time bounds $B \ge 0$ the probability that $T$ exceeds $B$ is at most

    \begin{equation}
        \Pr[T \leq B] \leq e^{\lambda(a - b)} \cdot B \cdot D \cdot p
    \end{equation}
\end{theorem}

To prove this theorem we use the following lemma from \cite{Hajek 82}

\begin{lemma}[Lemma 2.8 in \cite{Hajek 82}] \label{lm:haj}
    If conditions 2 and 3 are met then for all $t \geq 0$ we have

    $$\Pr[\gfun(X_t) \leq a] \leq \rho^te^{\lambda(a - \gfun(X_0))} + \frac{1 - \rho^t}{1 - \rho} D e^{\lambda(a - b)}$$

\end{lemma}

\begin{proof}[Proof of Theorem \ref{thm:neat}]

    If $T = B$, then we have $\gfun(X_k) \leq a$ and for all $t < B$ we have $\gfun(X_t) > a$. Hence, we compute

    \begin{align*}
        \Pr[T \leq B] & = \sum_{k=1}^{B} \Pr[T = k]                                                                        \\
                      & = \sum_{k=1}^{B} \Pr[\gfun(X_k) \leq a \land \gfun(X_{k - 1}) > a \land ... \land \gfun(X_0) > a)] \\
                      & \leq \sum_{k=1}^{B} \Pr[\gfun(X_k) \leq a].
    \end{align*}

    By Lemma $\ref{lm:haj}$

    \begin{align*}
        \Pr[T \leq B] & \leq \sum_{k=1}^{B} \left(\rho^k e^{\lambda(a - \gfun(X_0))} + \frac{1 - \rho^k}{1 - \rho} D e^{\lambda(a - b)}\right)                                                          \\
                      & = \rho \left(\frac{1 - \rho^B}{1 - \rho}\right) e^{\lambda a}\left(e^{-\lambda\gfun(X_0)} - \frac{D}{1 - \rho} e^{-\lambda b}\right) + e^{\lambda(a - b)} \frac{B D}{1 - \rho}.
    \end{align*}

    Since $\gfun(X_0) \ge b$ we have

    \begin{align*}
        \Pr[T \leq B] & \leq \rho \frac{1 - \rho^B}{1 - \rho} e^{\lambda (a - b)}\left(1 - \frac{D}{1 - \rho}\right) + e^{\lambda(a - b)} \frac{B D}{1 - \rho}
    \end{align*}

    Since $\frac{D}{1 - \rho} \geq D \geq 1 $, we further compute
    \begin{align*}
        \Pr[T \leq B] \leq e^{\lambda(a - b)} \frac{B D}{1 - \rho} = e^{\lambda(a - b)} \cdot B \cdot D \cdot p
    \end{align*}

\end{proof}
\subsection{Theorem for sub-gaussian processes}
\cm
\subsection{Comparing requirements}
\label{sec:compare}
To use some versions of drift theorems, we need some requirements on the sequence of random variables to be satisfied, such as the following:
$$
    \exists \delta \geq 0,\ c \in \mathbb{R} \ \forall  t \geq 0 \ \forall \gamma \in [0, \delta] $$$$
    E\left[e^{\gamma (X_{t + 1} - X_{t})}\right] \leq e^{\frac{c \gamma^2}{2}},$$

оr

$$\exists \delta \geq 0,\ p \geq 1 \ \forall t \geq 0 $$$$
    E\left[e^{\gamma (X_{t + 1} - X_{t})}\right] \leq 1 - \frac{1}{p}\ .$$


To compare rigors of requirements we need to explore behavior $E[e^{\gamma X}]$ regarding to $\gamma$. Of course this behavior depends on distribution type, hence consider common facts.

\subsubsection*{Continuos distribution}

Consider the definition of expectation of a function
\[
    E[e^{\gamma X}] = \int_{-\infty}^{+\infty} f(x) e^{\gamma x} \,dx,
\]

where $f(x)$ is a probability density function.

\hfill

Let's take the first derivative $\gamma$

\begin{align*}
    \frac{\partial E[e^{\gamma X}]}{\partial \gamma} & = \frac{\partial}{\partial \gamma} \int_{-\infty}^{+\infty} f(x) e^{\gamma x} \, dx = \int_{-\infty}^{+\infty} f(x) \frac{\partial e^{\gamma x}}{\partial \gamma} \, dx = \int_{-\infty}^{+\infty} x f(x) e^{\gamma x} \, dx = \\
                                                     & = \int_{-\infty}^{0} x f(x) e^{\gamma x} \, dx + \int_{0}^{+\infty} x f(x) e^{\gamma x} \, dx.
\end{align*}

Note that $\forall x \leq 0 e^{\gamma x} \leq 1$ and $\forall x \geq 0 e^{\gamma x} \geq 1 + \gamma x$. Hence because $\forall x f(x) \geq 0$

\begin{align*}
    \int_{0}^{+\infty} x f(x) e^{\gamma x} \, dx & \geq \int_{0}^{+\infty} x f(x) (1 + \gamma x)\, dx \\
    \int_{-\infty}^{0} x f(x) e^{\gamma x} \, dx & \geq \int_{-\infty}^{0} x f(x)\, dx.
\end{align*}

And then

\begin{align*}
    \frac{\partial E[e^{\gamma X}]}{\partial \gamma} & \geq  \int_{0}^{+\infty} x f(x) (1 + \gamma x)\, dx + \int_{-\infty}^{0} x f(x)\, dx \\
                                                     & = E[X] + \gamma \int_{0}^{+\infty} x^2 f(x) \, dx.
\end{align*}

Denote $\int_{0}^{+\infty} x^2 f(x) \, dx = c \geq 0$ and then if we reintegrate back, we get a restriction on the expectation from below

\[
    E[e^{\gamma X}] = E[1] + \int_0^\gamma \frac{\partial E[e^{\gamma X}]}{\partial \gamma} \, d\gamma \geq 1 + \gamma E[X] + \gamma^2 \frac{c}{2}.
\]

\subsubsection*{Discrete distribution}

In the same way consider expectation of function by definition

\begin{align*}
    E[e^{\gamma X}] = \sum_{i = 1}^{+\infty} e^{\gamma x_i} \Pr[x = x_i],
\end{align*}

where $\Pr$ is a probability function.

% \hfill

Denote set $P = \{i \in \mathbb{N}: x_i \geq 0\}$. Hence

\begin{align*}
    E[e^{\gamma X}] = \sum_{i = 1}^{+\infty} e^{\gamma x_i} Pr[x = x_i] = \sum_{i \in P} e^{\gamma x_i} Pr[x = x_i] + \sum_{i \in \mathbb{N}\setminus P} e^{\gamma x_i} Pr[x = x_i],
\end{align*}

After taking the first derivative we may split terms from $P$ set with other terms
% В таком случае, аналогично возьмем производную и рассмотрим пот отдельности слагаемые из множества P и остальные:

\begin{align*}
    \der{E[e^{\gamma X}]}{\gamma} & = \sum_{i = 1}^{+\infty} \frac{\partial e^{\gamma x_i}}{\partial \gamma} Pr[x = x_i] = \sum_{i = 1}^{+\infty} x_i e^{\gamma x_i} Pr[x = x_i] \\
                                  & = \sum_{i \in P} x_i e^{\gamma x_i} Pr[x = x_i] + \sum_{i \in \mathbb{N}\setminus P} x_i e^{\gamma x_i} Pr[x = x_i]                          \\
                                  & \geq \sum_{i \in P} x_i (1 + {\gamma x_i}) Pr[x = x_i] + \sum_{i \in \mathbb{N}\setminus P} x_i Pr[x = x_i]                                  \\
                                  & = E[X] + \gamma\sum_{i \in P} x_i^2 Pr[x = x_i]
\end{align*}

Again denote $c = \sum_{i \in P} x_i^2 Pr[x = x_i] \geq 0$ and obtain a result similar to the previous paragraph

\[
    E[e^{\gamma X}] \geq 1 + \gamma E[X] + \gamma^2 \frac{c}{2}
\]

Because formulas for both cases are same all further conclusions will be same too.

\subsubsection{Impracticability for positive drift}

In this case, the expectation of the exponent cannot be lower than 1, and also in the vicinity of zero it grows faster than any$e^{\frac{\gamma^2 c}{2}}$, because

$$\frac{\partial e^{\frac{\gamma^2 c}{2}}}{\partial \gamma} = c\gamma e^{\frac{\gamma^2 c}{2}} = g(\gamma),$$

But $g(0) = 0$, while $E[X] > 0$, therefore, that in any vicinity of zero

$$\forall c > 0 \, \exists \gamma_c > 0 \, \forall \gamma \in (0, \gamma_c) e^{\frac{\gamma^2 c}{2}} < E[e^{\gamma X}],$$

Hence both of requirements are not feasible.

\subsubsection{Zero drfit case}
\label{sec:zero}

In this section we will consider some specific family of continuos distributions with symmetric probability density functions, because their expectation equals 0.

\paragraph*{Exponent of polynomial}

Consider function

$$f(x) = c \expx{a},\ a > 0.$$

Compute $c$

\begin{align*}
    \infint{f(x)} & = \infint{c\expx{a}} = 1
\end{align*}

\begin{align*}
    \infint{\expx{a}} = 2\infintpoz{\expxpoz{a}}.
\end{align*}

Let's make a replacement

\begin{align*}
    u  & = x^a                                              \\
    du & = dx\,(a x^{a - 1}) = dx\,(a u^{1 - \frac{1}{a}}),
\end{align*}

then

\begin{align*}
    2\infintpoz{\expxpoz{a}} = \frac{2}{a} \infintpoz{e^{-u}u^{\frac{1}{a} - 1}} = \frac{2\Gamma(\frac{1}{a})}{a}.
\end{align*}

We conclude that $c = \frac{a}{2\Gamma(\frac{1}{a})}
$ and

$$f(x) = \frac{a}{2\Gamma(\frac{1}{a})} \expx{a}.$$

Let's find n-th momentum of $X$ (expectation от $X^n$)

\begin{align*}
    E[X^n] = \infint{f(x) x^n} = \frac{\Gamma(\frac{n + 1}{a})}{2\Gamma(\frac{1}{a})} (1 + (-1)^n)
\end{align*}

Then we can consider $e^{\gamma X}$ as a infinite series

\begin{align*}
    E[e^{\gamma X}] = E\left[\sum_{n = 0}^{+\infty}\frac{(\gamma X)^n}{n!}\right] & = \sum_{n = 0}^{+\infty}\gamma^n\frac{E[X^n]}{n!} = \sum_{n = 0}^{+\infty}\gamma^{2n}\frac{E[X^{2n}]}{2n!} =                                \\
    & = \sum_{n = 0}^{+\infty}\frac{(\gamma^2)^{n}}{n!}\left(\frac{E[X^{2n}]n!}{2n!}\right) = \sum_{n = 0}^{+\infty}\frac{(\gamma^2)^{n}}{n!} a_n
\end{align*}

Third equality is satisfied because all odd momentums are 0 due to the symmetry of the function.

Consider sequence $\{\sqrt[n]{a_n}\}$

\begin{align*}
    A = \lim_{n \to +\infty} \sqrt[n]{a_n} & = \lim_{n \to +\infty} \sqrt[n]{\frac{n! E[X^{2n}]}{2n!}} =
    \lim_{n \to +\infty} \sqrt[n]{
        \frac{
            \left(\frac{n}{e}\right)^n \sqrt{2\pi n} \
            \Gamma\left(\frac{2n + 1}{a}\right)}{
            \left(\frac{2n}{e}\right)^{2n} \sqrt{4\pi n}\ \Gamma\left(\frac{1}{a}\right)}
    } \\
    & = \lim_{n \to +\infty} \frac{n}{e} \frac{e^2}{4n^2} \sqrt[n]{\Gamma\left(\frac{2n + 1}{a}\right)} \simeq \lim_{n \to +\infty}  \frac{e}{4n} \sqrt[n]{\left(\frac{2n + 1}{ae}\right)^{\frac{2n + 1}{a}}} \\
    & = \lim_{n \to +\infty} \frac{e}{4n} \left(\frac{2n}{ae}\right)^{\frac{2}{a}} = \left(\frac{e^{1 - \frac{2}{a}}}{2^{2 - \frac{2}{a}} a^\frac{2}{a}}\right) \lim_{n \to +\infty} n^{\frac{2}{a} - 1}.
\end{align*}

Let's take a look on convergence of previous sequence regarding to values of the $a$
\begin{caseof}
    \case{$a \geq 2$}{

        Then $A$ exist, which means existing $C > 0$ with property
        $\forall n \geq 0 a_n \leq C^n$.
        Hence $E[e^{\gamma X}] \leq e^{\gamma^2 C}.$
    }
    \case{$a < 2$}{

        Sequence doesn't converge, so you can't say anything about sub-gausality.
    }
\end{caseof}

We can compute same limit for $\{\sqrt[n]{\frac{a_n}{n!}}\}$ and threshold regarding to $a$ is 1, where only for $a \geq 1$ series converge.

In this case we need to explore behavior of series when $a \in (1, 2)$. By another definition of sub-gaussian process

$$\exists K > 0 \ \forall p \geq 1 \left(E[X^p]\right)^{\frac{1}{p}} \leq K \sqrt{p}.$$


Let's compute (assume $p\ \vdots\ 2$)

\begin{align*}
    E[X^p]             & = \frac{\Gamma\left(\frac{p + 1}{a}\right)}{\Gamma\left(\frac{1}{a}\right)}                                     \\
    E[X^p]^\frac{1}{p} & \underrel{p \to +\infty} {=} \left(\frac{p}{e a}\right)^{\frac{p}{a} \frac{1}{p}} = \mathcal{O}(p^\frac{1}{a}).
\end{align*}

For $a \in (1, 2)$ obviously that asymptotics is greater that square root. Hence sub-gausality is not feasible.

\subsubsection{Processes with negative drift}

In this case, similarly to the positive drift case, it is the summand equal to the expectation that makes the main contribution to the derivative in some neighborhood of zero, since

$$\frac{\partial E[e^{\gamma x}]}{\partial \gamma} \Big|_{\gamma = 0} = E[X] < 0.$$

So both requirements are feasible since

$$\exists \gamma_0 > 0 \forall \gamma \in (0, \gamma_0) E[e^{\gamma X}] < 1.$$

So proving that the process is sub-gaussian is tantamount to finding $p$ in the second requirements.

\textbf{*РАССУЖДЕНИЯ ПРО П И КАРТИНКИ*}

Define function for distribution $X$ $B_X$ such that
\begin{align*}
    B_X(\gamma) = 1 + \gamma E[X] + \frac{\gamma^2}{2} E[X^2]_+,
\end{align*}

Where
\[
E[X^2]_+= 
\begin{cases}
    \int_0^{+\infty} x^2 f(x)\, dx,& \text{if $X$ is continuos}\\
    \sum_{x \geq 0} x^2 \Pr[X = x],              & \text{otherwise}.
\end{cases}
\]

Also define function $Ee_X(\gamma) = E[e^{\gamma X}]$. We proofed in end of section \ref*{sec:compare} that

\begin{align*}
    \forall \gamma\geq 0: Ee_X(\gamma) \geq B_X(\gamma).
\end{align*}

If $Ee(\gamma)$ is a differentiable function, then by definition the derivative will be continuos. Since $e^{\gamma x}$ and $f(x)$ is non-negative for all $x$ and $\gamma$, hence

\begin{align*}
    \frac{\partial^2 Ee(\gamma)}{\partial\gamma^2} = \infint{x^2 f(x) e^{\gamma x}} > 0.
\end{align*}

Then
\begin{align*}
    \der{Ee(\gamma)}{\gamma} \Big|_{\gamma = \gamma_0} - \der{Ee(\gamma)}{\gamma}\Big|_{\gamma = 0} &= \int_0^{\gamma_0} \frac{\partial^2 Ee(\gamma)}{\partial\gamma^2}\, d\gamma > 0 \\
    \der{Ee\left(\gamma_0\right)}{\gamma} &= E[X] + \int_0^{\gamma_0} \frac{\partial^2 Ee(\gamma)}{\partial\gamma^2}\, d\gamma
\end{align*}

Since second derivative always great than zero, than first derivative is continuously growing.

Then if $E[X] < 0$ $Ee(\gamma)$ would go down in some neighborhood. And if $E[X^2]_+ \neq 0$ it will have point with zero derivative, what fits the condition of the second requirement.

\pagebreak

Example for distribution $X$ with probability density function $f(x) = 0.65 e^{-0.65(x + 2)}$:

\begin{figure}[h!]
    \begin{center}
     \begin{tikzpicture}[scale = 0.8]
        \begin{axis}[
            axis lines = left,
            xlabel = \(\gamma\),
            ylabel = {\(f(\gamma)\)},
            xmin=0, xmax=1,
            ymin=0.9, ymax=1.2
        ]
        \addplot[color=darkGreen,
            domain=0:0.9,
            dashed,
            line width=1pt
        ]{1 + (-0.4615) * x + (1.2901) / 2 * x * x};
        \addlegendentry{\(B_X(\gamma)\)}

        \addplot[color=darkBlue,
            domain=0:0.6,
            line width=1.2pt
        ]{-0.65/(x - 0.65) * exp(-2 * x)};
        \addlegendentry{\(Ee_X(\gamma)]\)}

        \addplot[color=black,mark=*] coordinates {(0.3578, 0.9174)};
        \addplot[color=black,mark=*] coordinates {(0.15, 0.9631)};
        \addplot[color=red,
            domain=0:3,
            dashed
        ]{1};
        \end{axis}
     \end{tikzpicture}
    \end{center}
    % \caption{Plot of the function $Ee_X(\gamma)$ with its bound $B_X(\gamma)$, where $X$ has probability density function $f(x) = 0.65 e^{-0.65(x + 2)}$.}
    % \label{fig:negativeDriftPic2}
\end{figure}

If $E[X] > 0$ then in some neighborhood around $\gamma = 0$ would be greater than any $e^{\frac{c \gamma^2}{2}}$, because if

$$\frac{\partial e^{\frac{\gamma^2 c}{2}}}{\partial \gamma} = c\gamma e^{\frac{\gamma^2 c}{2}} = g(\gamma),$$

$g(0) = 0 < E[X]$ and $Ee_X(0) = B_X(0) = 1$. Hence both of requirements are not feasible.

Example 

\begin{figure}[h!]
    \begin{center}
     \begin{tikzpicture}[scale = 0.8]
        \begin{axis}[
            axis lines = left,
            xlabel = \(\gamma\),
            ylabel = {\(f(\gamma)\)},
            xmin=0, xmax=0.4,
            ymin=0.95, ymax=1.5
        ]
        % \addplot[color=darkGreen,
        %     domain=0:0.9,
        %     dashed,
        %     line width=1pt
        % ]{1 + (1) * x + (1.925) / 2 * x * x};
        % \addlegendentry{\(B_X(\gamma)\)}
        \addplot[color=green,
            domain=0.04:0.3,
            line width=0.8pt
        ]{exp((50 * x^2) / 2)};

        \addplot[color=red,
            domain=0:0.6,
            line width=0.8pt,
            dashed
        ]{exp((4 * x^2) / 2)};

        \addplot[color=red,
            domain=0:0.6,
            line width=0.8pt,
            dashed
        ]{exp((10 * x^2) / 2)};

        \addplot[color=green,
            domain=0.1:0.6,
            line width=0.8pt
        ]{exp((20 * x^2) / 2)};

        % \addplot[color=red,
        %     domain=0:0.6,
        %     line width=1.2pt
        % ]{exp((15 * x^2) / 2)};

        \addplot[color=red,
            domain=0:0.6,
            line width=0.8pt,
            dashed
        ]{exp((20 * x^2) / 2)};

        \addplot[color=red,
            domain=0:0.3,
            line width=0.8pt,
            dashed
        ]{exp((50 * x^2) / 2)};

        \addplot[color=darkBlue,
            domain=0:0.6,
            line width=1.2pt
        ]{exp((x^2 + 2*x) / 2)};
        
        \addlegendentry{\(Ee_X(\gamma)\)}

        % \addplot[color=black,mark=*] coordinates {(0.3578, 0.9174)};
        % \addplot[color=black,mark=*] coordinates {(0.15, 0.9631)};
        \addplot[color=red,
            domain=0:3,
            dashed
        ]{1};
        \end{axis}
     \end{tikzpicture}
    \end{center}
    % \caption{Plot of the function $Ee_X(\gamma)$ with its bound $B_X(\gamma)$, where $X$ has probability density function $f(x) = 0.65 e^{-0.65(x + 2)}$.}
    % \label{fig:negativeDriftPic2}
\end{figure}

\subsection*{Заключение}

In the case of $\textbf{\text{zero}}$ expectation we cannot always guarantee subgausality, which has been proved by the example of series of even functions $\expx{a}$.

In the case of $\textbf{\text{positive}}$ expectation both criteria are not feasible, and if it is $\textbf{\text{negative}}$, then both are feasible.

\section{Tail bounds}
\subsection{Upper bounds}
Consider a process $\{X_t\}_{t \in \mathbb{N}}$ with positive drift (i.e. $E[X_{t + 1} - X_t |\ X_t = s] \geq 0$ for all $s$) and another process $\{Y_t\}_{t \in \mathbb{N}}$ such that $Y_t = X_t - \varepsilon t$ and it has negative drift (i.e. $E[Y_{t + 1} - Y_t |\ Y_t = s] \leq 0$ for all $s$).

Our aim is to bound the probability $\Pr[T_X \leq t_0]$, where $T_X$ is the first time when $X_t \geq b$ for some $b > X_0$ and for $t_0$. We first note that

\begin{align*}
    \Pr[T_X \leq t_0] \leq \Pr[X_{t_0} \geq b] \leq \Pr[Y_{t_0} \geq b - \varepsilon t_0].
\end{align*}

Since $\{Y_t\}_{t \in \mathbb{N}}$ is a process with negative drift, it is a subject to Theorem \cite{}. Hence, we have

\begin{align*}
    \Pr[Y_{t_0} \geq b - \varepsilon t_0] \leq t_0 D p e^{-\gamma(b - \varepsilon t_0 - a)},
\end{align*}

which also implies

\begin{align*}
    \Pr[T_X \leq t_0] \leq t_0 D p e^{-\gamma(b - \varepsilon t_0 - a)}.
\end{align*}

Let $t_0 := \frac{\kappa (b - a)}{\varepsilon}$. Then we compute
\begin{align*}
    \Pr\left[T_X \leq \frac{\kappa (b - a)}{\varepsilon}\right] \leq \frac{\kappa (b - a)}{\varepsilon} D p e^{-\gamma(1 - \kappa)(b - a)}.
\end{align*}

For example we can use $\kappa = \frac{1}{2}$ and obtain

\begin{align*}
    \Pr\left[T_X \leq \frac{(b - a)}{2\varepsilon}\right] \leq \frac{(b - a)}{2\varepsilon} D p e^{-\gamma\frac{(b - a)}{2}}.
\end{align*}

\subsection{Lower bounds}

Consider a process $\{X_t\}_{t \in \mathbb{N}}$ with positive drift (i.e. $E[X_{t + 1} - X_t |\ X_t = s] \geq 0$ for all $s$) and another process $\{Y_t\}_{t \in \mathbb{N}}$ such that $Y_t = \varepsilon t - X_t$ and it has negative drift (i.e. $E[Y_{t + 1} - Y_t |\ Y_t = s] \leq 0$ for all $s$).

Our aim is to bound the probability $\Pr[T_X > t_0]$, where $T_X$ is the first time when $X_t \geq b$ for some $b > X_0$ and for $t_0$. We first note that

\begin{align*}
    \Pr[T_X > t_0] \leq \Pr[X_{t_0} < b] \leq \Pr[Y_{t_0} > \varepsilon t_0 - b].
\end{align*}

Since $\{Y_t\}_{t \in \mathbb{N}}$ is a process with negative drift, it is a subject to Theorem \cite{}. Hence, we have

\begin{align*}
    \Pr[Y_t > \varepsilon t_0 - b] \leq t_0 D p e^{-\gamma(\epsilon t_0 - b + a)},
\end{align*}

which also implies

\begin{align*}
    \Pr[T_X > t_0] \leq t_0 D p e^{-\gamma(\epsilon t_0 - b + a)}.
\end{align*}

Let $t_0 := \frac{\kappa (b - a)}{\varepsilon}$. Then we compute
\begin{align*}
    \Pr\left[T_X > \frac{\kappa (b - a)}{\varepsilon}\right] \leq \frac{\kappa (b - a)}{\varepsilon} D p e^{-\gamma(\kappa - 1)(b - a)}.
\end{align*}

For example we can use $\kappa = \frac{3}{2}$ and obtain

\begin{align*}
    \Pr\left[T_X > \frac{3 (b - a)}{2\varepsilon}\right] \leq \frac{3(b - a)}{2\varepsilon} D p e^{-\gamma\frac{(b - a)}{2}}.
\end{align*}

% Lets take a look on process $\{X_t\}$ and its derivative process $\{Y_t\}$, where $Y_t = \varepsilon t - X_t$ and it has negative drift.

% Then we want to bounds $\Pr[T_X > t_0]$, where $T_X$ - first time $X_t \geq b$. 

% \begin{align*}
%     \Pr[T_X > t_0] \leq \Pr[X_{t_0} < b] \leq \Pr[Y_{t_0} > \varepsilon t_0 - b]
% \end{align*}

% Then, because $\{Y_t\}$ - process with negative drift, we may use the corresponding theorem *label*:

% \begin{align*}
%     \Pr[Y_t > \varepsilon t_0 - b] \leq t_0 D p e^{-\gamma(\epsilon t_0 - b + a)}
% \end{align*}

% Lets rewrite some to $T_X$: 

% \begin{align*}
%     \Pr[T_X > t_0] \leq t_0 D p e^{-\gamma(\epsilon t_0 - b + a)}
% \end{align*}

% Replace $t_0$ with another expression:

% \begin{align*}
%     t_0 := \frac{\kappa (b - a)}{\varepsilon}
% \end{align*}

% And then:

% \begin{align*}
%     \Pr\left[T_X > \frac{\kappa (b - a)}{\varepsilon}\right] \leq \frac{\kappa (b - a)}{\varepsilon} D p e^{-\gamma(\kappa - 1)(b - a)}
% \end{align*}

% For example use $\kappa = \frac{3}{2}$:

% \begin{align*}
%     \Pr\left[T_X > \frac{3 (b - a)}{2\varepsilon}\right] \leq \frac{3(b - a)}{2\varepsilon} D p e^{-\gamma\frac{(b - a)}{2}}
% \end{align*}

% ПОВТОРИТЬ ЕЩЕ РАЗ ПРЕДЫДУЩИЙ ПАРАГРАФ

\section{Time bounds for process with variable $\lambda$}
\cm
\subsection{Case when $\lambda$ is random value}
Consider the total progress $\Delta X$ by $T$ iterations. It is a sum of all progresses $\Delta X_i$ made in previous iteration
\[
    \Delta X = \sum_{i = 0}^{T} \Delta X_i
\]

By Vald's equality we have

\[
    E[\Delta X] = E\left[\sum_{i = 1}^{T} \Delta X_i\right].
\]

If ew assume that $E[\Delta X_i]$ is a constant or at least we can give relatively sharp bounds lower and upper bounds on it, then
\[
    E[\Delta X] = E\left[\sum_{i = 1}^{T} \Delta X_i\right] \thickapprox E[T] E[\Delta X_i].
\]

Let $\Lambda$ be the total cost, which is total number of fitness evaluations (the fitness evaluations in iteration $i$ equals $\lambda_i$). Since $\lambda$ is random variable, each $\lambda_i$ has the same expectation. Therefore, we have
\[
    E\left[\Lambda\right] = E\left[\sum_{i = 1}^{T} \lambda_i\right] = E[T] E[\lambda] \thickapprox \frac{E[\Delta X] E[\lambda]}{E[\Delta X_i]}
\]

If we denote $\vartheta_\lambda = \frac{E[\Delta X_i]}{E[\lambda]}$, which is the expected progress per fitness evaluation, then the previous equation is simplified to
\[
    E[\Lambda] \thickapprox \frac{E[\Delta X]}{\vartheta_\lambda}
\]
% $\vartheta_{\lambda, X_t} = \nu (\lambda, X_t)$

\section{Conclusion}
\cm
\end{document}